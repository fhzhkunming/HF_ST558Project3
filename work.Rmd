---
title: "ST558_Project3"
author: "Hui and Joy"
date: "2023-11-02"
output: github_document
params:
      Edu: "SomeElementary"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Diabetes is a seriously pervasive chronic disease that disrupts the body’s ability to regulate blood glucose levels, leading to a diminished quality of life and reduced life expectancy. It stands as one of the most prevalent chronic illnesses in the United States, impacting millions of Americans annually and imposing a significant economic burden on the nation.  

In this project, we will use the `diabetes binary health indicators` dataset obtained from [Kaggle]( https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/) to conduct comprehensive exploratory data analysis (EDA) and develop predictive models. This dataset comprises 253,680 survey responses to the CDC’s BRFSS (Behavioral Risk Factor Surveillance System) from year 2015. The primary target variable, `Diabetes_binary`, offers binary classification, distinguishing between 0 for no diabetes, and 1 for prediabetes or diabetes. This dataset encompasses 21 feature variables and is not balanced. Detailed information of variable can be found [here](https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/?select=diabetes_binary_health_indicators_BRFSS2015.csv).  

Our analysis will primarily focus on a subset of key variables, including High blood pressure (HighBP), High cholesterol (HighChol), cholesterol check (CholCheck), Body Mass Index (BMI), Smoker, Fruits, Veggies, and Age.  
In our EDA phase, we will start by summarizing basic statistics visualizing variable frequencies. This will be followed by the exploration of correlations between variables and the creation of contingency tables to better understand the interplay of these factors.  

Based on the results from EDA, we will split the dataset into training (70%) and test (30%) subsets for each educational level. Subsequently, we will employ the training data to fit six distinct models, including a logistic regression, a LASSO logistic regression, a classification tree model, a random forest model, a `partial least squares model`, as well as a `regularized logistic regression model`. The performance of these models will be rigorously evaluated using the test dataset, and we will determine the most effective model for predicting diabetes outcomes.  
  

Description of variables in the data set:   
+ Diabetes_binary: 0 = no diabetes 1 = prediabetes or diabetes  
+ HighBP: High blood pressure  
+ HighChol: High cholesterol   
+ CholCheck: 0 = no cholesterol check in 5 years 1 = yes cholesterol check in 5 years  
+ BMI: Body Mass Index   
+ Smoker: Have you smoked at least 100 cigarettes in your entire life? 0 = no 1 = yes  
+ Stroke: 0 = no 1 = yes  
+ HeartDiseaseorAttack: coronary heart disease (CHD) or myocardial infarction (MI) 0 = no 1 = yes  
+ PhysActivity: physical activity in past 30 days - not including job 0 = no 1 = yes   
+ Fruits: Consume Fruit 1 or more times per day 0 = no 1 = yes  
+ Veggies: Consume Vegetables 1 or more times per day 0 = no 1 = yes  
+ HvyAlcoholConsump: (adult men >=14 drinks per week and adult women>=7 drinks per week) 0 = no 1 = yes  
+ AnyHealthcare: Health care coverage 0 = no 1 = yes  
+ NoDocbcCost: Was there a time in the past 12 months when you needed to see a doctor but could not because of cost? 0 = no 1 = yes  
+ GenHlth: in general your health is: scale 1-5 1 = excellent 2 = very good 3 = good 4 = fair 5 = poor  
+ MentHlth: days of poor mental health scale 1-30 days  
+ PhysHlth: physical illness or injury days in past 30 days scale 1-30  
+ DiffWalk: Do you have serious difficulty walking or climbing stairs? 0 = no 1 = yes  
+ Sex: 0 = female 1 = male  
+ Age: 13-level age category 1 = 18-24 2 = 25-29 3 = 30-34 4 = 35-39 5 = 40-44 6 = 45-49 7 = 50-54 8 = 55-59 9 = 60-64 10 = 65-69 11 = 70-74 12 = 75-79 13 = 80 or older  
+ Education: scale 1-6 1 = Never attended school or only kindergarten 2 = Grades 1 through 8 (Elementary) 3 = Grades 9 through 11 (Some high school) 4 = Grade 12 or GED (High school graduate) 5 = College 1 year to 3 years (Some college or technical school) 6 = College 4 years or more (College graduate)  
+ Income: scale 1-8 1 = less than $10,000 5 = less than $35,000 8 = $75,000 or more  

# Data
## Read in data
```{r, message=FALSE}
library(dplyr)
library(readr)
diabetes <- as_tibble(read.csv("diabetes_binary_health_indicators_BRFSS2015.csv", header = TRUE))
head(diabetes)
```

## Grouping education levels
```{r}
diabetes$Education <- ifelse(diabetes$Education %in% c(1, 2), "SomeElementary",
                         ifelse(diabetes$Education == 3, "SomeHighSchool", 
                              ifelse(diabetes$Education == 4, "HighSchool",
                                   ifelse(diabetes$Education == 5, "SomeCollege",
                                        ifelse(diabetes$Education == 6, "College", NA)))))

params$Edu
```

## Subsetting the dataset based on education level
```{r}
EducationData <- filter(diabetes, (Education == params$Edu))
```

# EDA
## Checking the missing values
```{r}
missing_values <- colSums(is.na(EducationData))
```

## Summary statistics for numeric variables
```{r, warning=FALSE}
library(dplyr)
# Filter only numeric variables
numeric_vars <- EducationData %>% select_if(is.numeric)
# Create a summary table
summary_table <- numeric_vars %>%
  sapply(function(x) {
    c( count = sum(!is.na(x)),
      mean = mean(x, na.rm = TRUE),
      std = sd(x, na.rm = TRUE),
      min = min(x, na.rm = TRUE),
      Q1 = quantile(x, 0.25, na.rm = TRUE),
      median = median(x, na.rm = TRUE),
      Q3 = quantile(x, 0.75, na.rm = TRUE),
      max = max(x, na.rm = TRUE)
    )
  })
# Transpose the summary table
transposed_summary_table <- t(round(summary_table, 2)) 
colnames(transposed_summary_table) <- c("count", "mean", "std", 
                                        "min", "Q1", "median", "Q3", "max")
# Print the transposed table
knitr::kable(transposed_summary_table)
```
The center and spread of each variable can be found in this table.

## Check the correlation between `Diabetes_binary` and the other variables
```{r}
library(knitr)
# Create a correlation matrix between variables
Cor_Matrix <- EducationData %>% 
         select(Diabetes_binary, BMI, GenHlth, MentHlth, PhysHlth, Age, Income) %>%
         cor()

# Round the correlation matrix to two decimal places
rounded_Cor_Matrix <- round(Cor_Matrix, digits = 2)
# Print the rounded correlation matrix
kable(rounded_Cor_Matrix)
```

## Visualization of correlation with `Diabetes_binary` through bar graph
```{r, warning=FALSE}
library(ggplot2)
# Exclude the character variable from the data
data <- EducationData[, !(names(EducationData) %in% "Education")]

# Calculate the correlation of each variable with 'Diabetes_binary'
correlations <- sapply(data[-1], function(x) cor(x, data$Diabetes_binary))

# Create a data frame for the correlations
correlation_data <- data.frame(Variable = names(correlations), Correlation = correlations)

# Create a bar chart of correlations
ggplot(correlation_data, aes(x = Variable, y = Correlation)) +
  geom_bar(stat = "identity", fill = "orange") +
  labs(title = "Correlation with Diabetes_binary", x = "Variable", y = "Correlation") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
  # coord_flip()  # Rotate the x-axis labels for better readability
```

In this chart, some of the variables exhibited positive correlations with 'Diabetes_binary,' while others showed negative correlations with it.

## Convert some variables to factor
```{r}
EducationData$Diabetes_binary <- factor(EducationData$Diabetes_binary, 
                                       levels = c(0, 1),
                                       labels = c("NonDiabetes", "Diabetes"))
# EducationData$Income <- as.factor(EducationData$Income)
EducationData$HighBP <- factor(EducationData$HighBP,
                               levels = c(0, 1),
                               labels = c("NonHighBP", "HighBP"))
EducationData$HighChol <- factor(EducationData$HighChol,levels = c(0, 1),
                                 labels = c("NonHighChol", "HighCholesterol"))
EducationData$Sex <- factor(EducationData$Sex,levels = c(0, 1), 
                            labels = c("Female", "Male"))
EducationData$Fruits <- factor(EducationData$Fruits,levels = c(0, 1), 
                              labels = c("No", "Yes"))
EducationData$Veggies <- factor(EducationData$Veggies,levels = c(0, 1),
                                labels = c("No", "Yes"))
EducationData$CholCheck <- as.factor(EducationData$CholCheck)
EducationData$Smoker <- factor(EducationData$Smoker, levels = c(0, 1),
                                labels = c("No", "Yes"))
EducationData$Stroke <- as.factor(EducationData$Stroke)
EducationData$HeartDiseaseorAttack <- as.factor(EducationData$HeartDiseaseorAttack)
EducationData$PhysActivity <- factor(EducationData$PhysActivity, levels = c(0, 1),
                                       labels = c("No", "Yes"))
EducationData$HvyAlcoholConsump <- as.factor(EducationData$HvyAlcoholConsump)
EducationData$AnyHealthcare <- as.factor(EducationData$AnyHealthcare)
EducationData$NoDocbcCost <- as.factor(EducationData$NoDocbcCost)
EducationData$DiffWalk <- factor(EducationData$DiffWalk, levels = c(0, 1),
                                       labels = c("No", "Yes"))
```

## Summary statistics for Character variables
```{r}
library(knitr)
# one-way table
kable(table(EducationData$Diabetes_binary))
```

```{r}
kable(table(EducationData$HighBP))
```

```{r}
# two-way table
kable(table(EducationData$Diabetes_binary, EducationData$HighChol))
```

```{r}
kable(table(EducationData$Diabetes_binary, EducationData$Fruits))
```

```{r}
kable(table(EducationData$Diabetes_binary, EducationData$Veggies))
```

```{r}
kable(table(EducationData$Diabetes_binary, EducationData$Sex))
```

```{r}
# Three-way table
kable(table(EducationData$Diabetes_binary, EducationData$HighBP, EducationData$Sex))
```

## Graphical Summaries
### Bar plots
```{r}
library(ggplot2)
library(cowplot)
# Checking The relation B/W HighBP and Diabetes
g1 <- ggplot(data = EducationData, aes(x = HighBP, y = after_stat(count), 
                                       fill = Diabetes_binary)) +
             geom_bar(position = "dodge") +
             labs(title = "Diabetes Frequency for High Blood Pressure",
                  x = "High Blood Pressure",
                  y = "Frequency") + theme_minimal() +
            theme(text = element_text(size = 8)) 
# Checking The relation B/W HighChol and Diabetes
g2 <- ggplot(data = EducationData, aes(x = HighChol, y = after_stat(count), 
                                       fill = Diabetes_binary)) +
             geom_bar(position = "dodge") +
               labs(title = "Diabetes Frequency for High Cholesterol",
                    x = "High Cholesterol",
                    y = "Frequency") + theme_minimal() +

            theme(legend.position = "none",
                  text = element_text(size = 8)) # Hide the legend
g3 <- ggplot(data = EducationData, aes(x = DiffWalk, y = after_stat(count), 
                                       fill = Diabetes_binary)) +
             geom_bar(position = "dodge") +
             labs(title = "Diabetes Frequency for Difficult Walking",
                  x = "Difficult Walking",
                  y = "Frequency") + theme_minimal() +
            theme(legend.position = "none",
                  text = element_text(size = 8)) # Hide the legend
g4 <- ggplot(data = EducationData, aes(x = Smoker, y = after_stat(count), 
                                       fill = Diabetes_binary)) +
             geom_bar(position = "dodge") +
             labs(title = "Diabetes Frequency for Smoker",
                  x = "Smoker",
                  y = "Frequency")  + theme_minimal() +
            theme(legend.position = "none",
                  text = element_text(size = 8)) # Hide the legend           
# Arrange the plots into a 2x2 gird using cowplot
combined_plots <- plot_grid(g1, g2, g3, g4,
                            ncol = 2,
                            labels = c("a", "b", "c", "d"))
# Print the combined plots
combined_plots
```

```{r}
library(ggplot2)
# Define labels for 'GenHlth'
genhlth_labels <- c("Excellent", "Very Good", "Good", "Fair", "Poor")

# Create a plot with two facets (subplots)
g5 <- ggplot(EducationData, aes(x = factor(GenHlth, levels = 1:5, labels = genhlth_labels), 
                               fill = Diabetes_binary)) +
  geom_bar(position = 'dodge', alpha = 0.5, width = 0.5) +
  facet_wrap(~ Diabetes_binary, ncol = 2) +
  labs(title = 'General Health Conditions Distribution', x = NULL, y = 'Count') +
  theme_minimal() +
  theme(legend.title = element_blank()) +
  theme(legend.position = "none", axis.text.x = element_text(size = 8))
g5
```
BMI distribution
```{r}
g6 <- ggplot(data = EducationData, aes(x = BMI, y = after_stat(count), 
                                       fill = Diabetes_binary)) +
             geom_bar() +
             labs(title = "BMI distribution",
                  x = "BMI",
                  y = "Count")  + theme_minimal() +
            theme(legend.position = "none",
                  text = element_text(size = 8)) 
g6
```

### Box plots
```{r}
library(gridExtra)
g7 <- ggplot(EducationData, aes(x = Diabetes_binary, y = BMI, fill = Diabetes_binary)) +
     geom_boxplot() +
     labs(title = "BMI Distribution vs. Diabetes",
       x = "Diabetes_binary",
       y = "BMI") +
     theme(text = element_text(size = 8)) 
g8 <- ggplot(EducationData, aes(x = Diabetes_binary, y = Age, fill = Diabetes_binary)) +
      geom_boxplot() +
  labs(title = "Age Distribution vs. Diabetes",
       x = "Diabetes_binary",
       y = "Age") +
  theme(text = element_text(size = 8)) 
g9 <- ggplot(EducationData, aes(x = Diabetes_binary, y = Income, fill = Diabetes_binary)) +
      geom_boxplot() +
  labs(title = "Income Distribution vs. Diabetes",
       x = "Diabetes_binary",
       y = "Income") +
  theme(text = element_text(size = 8))
g10 <- ggplot(EducationData, aes(x = Diabetes_binary, y = MentHlth, fill = Diabetes_binary)) +
      geom_boxplot() +
  labs(title = "Mental Health Distribution vs. Diabetes",
       x = "Diabetes_binary",
       y = "Mental Health") +
  theme(text = element_text(size = 8)) 
# Arrange the plots into a 2x2 gird using cowplot
complots <- plot_grid(g7, g8, g9, g10,
                            ncol = 2,
                            labels = c("a", "b", "c", "d"))
# Print the combined plots
complots
```
### Density plots
```{r}
g11 <- ggplot(EducationData, aes(x = Income)) +
           geom_density(adjust = 0.5, alpha = 0.5, 
                        aes(fill = Diabetes_binary), position = "stack") +
          labs(title = "Distribution of Income vs. Diabetes",
               x = "Diabetes_binary",
               y = "Density") +
          theme(legend.position = "none", 
                text = element_text(size = 8))
  
g12 <- ggplot(EducationData, aes(x = Age)) +
           geom_density(adjust = 0.5, alpha = 0.5, 
                        aes(fill = Diabetes_binary), position = "stack") +
          labs(title = "Distribution of Age vs. Diabetes",
               x = "Age",
               y = "Density") +
  theme(text = element_text(size = 8))
# Arrange the plots into a 2x2 gird using cowplot
complots <- plot_grid(g11, g12, ncol = 2,
                      labels = c("a", "b"))
# Print the combined plots
complots
```

# Modeling   
## Split the data  
```{r, message=FALSE}
library(caret)
library(lattice)

# set seed for reproducibility
set.seed(123)

# Create a data partition for training (70%) and testing (30%) data
intrain <- createDataPartition(y = EducationData$Diabetes_binary,
                                 p = 0.70,
                                 list = FALSE
                                 )
# Create the training data set using the partition
train_set <- EducationData[intrain,]

# Create the testing data set using he partition
test_set <- EducationData[-intrain,]
```  
## Summarize and fit models  
### What is **log loss** and why may we prefer it to things like accuracy?  
**Log loss**, also known as logistic loss or cross-entropy loss, is a loss function used in classification problems to evaluate the performance of a model, especially in scenarios with binary response variables. It quantifies the performance of a model by measuring the difference between the predicted probability distribution produced by the model and the actual probability distribution. Log loss is calculated as the negative logarithm of the predicted probability assigned to the true class label. It penalizes the model for incorrect predictions and a lower log loss value indicates better model predictive performance.  

**Accuracy**, on the other hand, is a metric used to evaluate classification model performance by measuring the percentage of correctly classified instances. While accuracy is a useful metric, it can be misleading in some cases. For example, in imbalanced datasets where one class significantly outweighs the other, a model that always predicts the majority class will yield high accuracy but may not be practically useful. In such cases, log loss can offer a more reliable metric as it considers the model’s predictions uncertainty.  

In summary, log loss is a preferred metric over accuracy when the data is imbalanced or when the cost of false positives and false negatives is different.  

### Logistic Regression Model  
- What is a **logistic regression** and why we apply it to this kind of data?  

A **logistic regression model** is a statistical method used for analyzing the relationship between a binary outcome variable and one or more predictor variables. It is commonly used in classification problems, where the goal is to predict one of two possible outcomes, typically represented as 0 and 1 or “Yes” and “No”.  

The relationship between response variable and the predictor variables is evaluated using the logistic function (also known as the sigmoid function), which ensures that the predicted probabilities fall between 0 and 1. The model parameters are estimated using a process called maximum likelihood estimation. The key output of a logistic regression model includes the coefficients (log-odds) associated with each predictor variable, which describe the strength and direction of their influence on the probability of the outcome.  

Logistic regression is designed to handle categorical dependent variables. In our situation, Diabetes_binary is a binary variable and is suitable for fitting logistic regression model.  

- Fit Logistic Regression Models
```{r}
library(caret)
library(Metrics)
# Fit the logistic regression models
log_reg1 <- train(Diabetes_binary ~ HighBP + HighChol + CholCheck + 
                    Smoker + Fruits + Veggies + Sex + Income, 
               data = train_set, 
               family = "binomial",
               method = "glm",
               preProcess = c("center", "scale"),
               trControl = trainControl(method = "cv", number = 5),
               metric = "Accuracy",  
               trace = FALSE
               )

log_reg2 <- train(Diabetes_binary ~ HighBP*HighChol*CholCheck + 
                    Smoker + Fruits + Veggies + Sex + Income, 
               data = train_set, 
               family = "binomial",
               method = "glm",
               preProcess = c("center", "scale"),
               trControl = trainControl(method = "cv", number = 5),
               metric = "Accuracy",  
               trace = FALSE
               )

log_reg3 <- train(Diabetes_binary ~ HighBP + HighChol + 
                    CholCheck + Smoker, 
               data = train_set, 
               family = "binomial",
               method = "glm",
               preProcess = c("center", "scale"),
               trControl = trainControl(method = "cv", number = 5),
               metric = "Accuracy",  
               trace = FALSE
               )

# Print the model results
log_reg1
log_reg2
log_reg3

# Test the model
predicted_test1 <- predict(log_reg1, newdata = test_set, type = "prob")
predicted_test2 <- predict(log_reg2, newdata = test_set, type = "prob")
predicted_test3 <- predict(log_reg3, newdata = test_set, type = "prob")

# convert Diabetes_binary factors to numeric and calculate log loss.
test_set$Diabetes_binary <- as.numeric(test_set$Diabetes_binary)
log_loss1 <- logLoss(test_set$Diabetes_binary, predicted_test1$Diabetes)
log_loss2 <- logLoss(test_set$Diabetes_binary, predicted_test2$Diabetes)
log_loss3 <- logLoss(test_set$Diabetes_binary, predicted_test3$Diabetes)

# Print the logLoss values
log_loss1
log_loss2
log_loss3
```  
Based on the model fitting results, we select the one with higher Accuracy and lower log loss values.

### LASSO Logistic Regression Model  
- What is a **LASSO logistic regression** and why we might try to use it over basic logistic regression?  

**LASSO** (`least absolute shrinkage and selection operator`) is a regression analysis method that performs both variable selection and regularization to enhance the prediction accuracy and interpretability of the resulting statistical model. It uses `glmnet` package in R to fit the model.  

The primary goal of LASSO regression is to find a balance between model simplicity and accuracy. It achieves this by adding a penalty term to the traditional linear regression model, which encourages sparse solutions where some coefficients are forced to be exactly zero. This feature makes LASSO particularly useful for feature selection, as it can automatically identify and discard irrelevant or redundant variables. That’s why we might try to use LASSO logistic regression over basic logistic regression.  

- Fit a LASSO Logistic Regression Model
```{r, warning=FALSE}
library(glmnet)

# Select variables for fitting the model
predictors <- c("HighBP", "HighChol", "CholCheck", 
                "BMI", "Smoker", "Stroke", "PhysActivity",
                "HvyAlcoholConsump", "GenHlth", "MentHlth", 
                "PhysHlth", "Sex", "Age", "Income"
)

# Create a train control object for cross-validation
ctrl <- trainControl(method = "cv", # Cross-validation method (k-fold)
                     number = 5, # Number of cross-validation folds
                     verboseIter = FALSE
)

lambdas <- c(seq(0, 2, length = 3))

# Fit a Lasso model
lasso_fit <- train(Diabetes_binary ~ .,
                   data = train_set[, c("Diabetes_binary", predictors)],
                   method = "glmnet",
                   trControl = ctrl,
                   tuneGrid = expand.grid(alpha = 1, lambda = 0), # Alpha = 1 for LASSO
                   metric = "logLoss" # Use logLoss metric for evaluation
)

# Predict probabilities on the test set
predicted_lasso <- predict(lasso_fit, newdata = test_set, type = "prob")

# Calculate LogLoss manually
log_loss <- logLoss(test_set$Diabetes_binary, predicted_lasso$Diabetes)

# Print the log loss value
log_loss
```

### Classification Tree Model  
- What is a **classification tree model** and why we might try to use it?  

A **classification tree model** is a type of supervised learning algorithm used in statistics, data mining, and machine learning. It is used to predict the value of a categorical response variable based on one or more predictor variables. The model is built by recursively partitioning the data into smaller and smaller subsets, with each partition being based on a different predictor variable. The result is a tree-like structure where each internal node represents a test on a predictor variable, each branch represents the outcome of the test, and each leaf node represents a predicted value for the response variable. The tree is constructed in such a way that the most important predictor variables are at the top of the tree, and the least important ones are at the bottom. The goal of the algorithm is to create a tree that is as small as possible while still accurately predicting the response variable.  

Classification trees are a powerful and flexible machine learning algorithm that can be used for both classification and regression problems. They are particularly useful when the data has non-linear relationships, variable interactions, or outliers, and when interpretability is important. However, classification trees also have limitations, such as being prone to overfitting when they become too complex. To address, ensemble techniques like random forests and gradient boosting are often used to to enhance their performance.  

- Fit a Classification Tree Model  
```{r}

```

### Random Forest 
- What is a **random forest** and why we might use it instead of a basic classification tree?  

A **random forest regression model**, also known as a random forest, is a versatile machine learning algorithm used for both regression and classification tasks. It is an ensemble technique that uses multiple decision trees to make predictions. In this model, each decision tree is trained on a different subset of the data, and the final output is the average of all the outputs of the individual decision trees. This technique helps to reduce overfitting and improve the accuracy of the model. Random forest can predict continuous values, such as stock prices, temperature, or sales figures, as well as performing classification variables.  

Random forests have several advantages over the basic decision trees: reduced overfitting, improved accuracy, less sensitive to outliers, and identifying the most important predictors and improving the interpretability of the model. Therefore, we prefer random forest over basic classification trees.  

- Fit a random forest model
```{r}

```

### Partial Least Squares Model  
- A summary of a **partial least squares model**  

A **partial least squares (PLS) model** is a supervised learning algorithm used to model the relationship between two sets of variables. The PLS model is used when there are multicollinearity, i.e., the explanatory variables are correlated, and high dimensionality in the data. PLS allows us to reduce the dimensionality of correlated variables and model the underlying, shared, information of those variables (in both dependent and independent variables). Moreover, PLS can model multiple outcome variables, which many statistics and machine learning models cannot directly deal with. 

- Fit a partial least squares model
```{r}

```  
### Regularized Logistic Regression Model

- A summary of **regularized logistic regression**  

A **regularized logistic regression model** is a variation of logistic regression that includes regularization techniques to prevent overfitting and improve the model's generalization ability. It's used for binary classification problems, where the goal is to predict one of two possible outcomes.  

Regularized logistic regression is particularly useful when dealing with high-dimensional data with many predictor variables. It helps to improve model performance, maintain model simplicity, and select the most relevant features. It's commonly used in machine learning and data analysis when dealing with classification problems, especially when overfitting is a concern.  

- Fit a regularized logistic regression model
```{r}

```

